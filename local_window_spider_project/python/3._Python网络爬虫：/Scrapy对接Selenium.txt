b'Scrapy\xe5\xaf\xb9\xe6\x8e\xa5Selenium'  http://mmbiz.qpic.cn/mmbiz_png/egDQPiaUh6ylU5mFvIAWV1Dl0a03DO63n1qy4AJsNAqb45Ery7Crgibr65WBxzatOI2bibLMeicwk4F4zC2yOcqvEQ/640?
   Scrapy抓取页面的方式和Requests库类似，都是直接模拟HTTP请求，因此如果遇到JavaScript渲染的页面Scrapy同样是无法抓取的，而在前文中我们抓取JavaScript渲染的页面有两种方式，一种是分析Ajax请求，找到其对应的接口抓取，Scrapy中同样可以用此种方式抓取；另一种是直接用Selenium或Splash模拟浏览器进行抓取，这种方式我们不需要关心页面后台发生了怎样的请求，也不需要分析渲染过程，我们只需要关心页面最终结果即可，可见即可爬，所以如果在Scrapy中可以对接Selenium话就可以处理任何网站的抓取了。<br style=""  />
本节我们来看一下Scrapy框架中如何对接Selenium，这次我们依然是抓取淘宝商品信息，抓取逻辑和前文中用Selenium抓取淘宝商品一节完全相同。
首先新建项目，名称叫做scrapyseleniumtest，命令如下：
    接下来我们初步实现Spider的start_requests()方法，实现如下：
 在这里关键字我们用KEYWORDS标识，定义为一个列表，最大翻页页码用MAX_PAGE表示，统一定义在setttings.py里面，定义如下：
 接下来我们就需要处理这些请求的抓取了，这次抓取不同，我们要对接Selenium进行抓取，在这里采用Downloader Middleware来实现，在Middleware里面的process_request()方法里面对每个抓取请求进行处理，启动浏览器并进行页面渲染，再将渲染后的结果构造一个HtmlResponse返回即可。
代码实现如下：
 这里可能我们有人可能会纳闷了，为什么通过实现这么一个Downloader Middleware就可以了呢？之前的Request对象怎么办？Scrapy不再处理了吗？Response返回后又传递给了谁来处理？
是的，Request对象到这里就不会再处理了，也不会再像以前一样交给Downloader下载了，Response会直接传给Spider进行解析。
这究竟是为什么？这时我们需要回顾一下Downloader Middleware的process_request()方法的处理逻辑，在前面我们也提到过，内容如下：
当process_request()方法返回Response对象的时候，接下来更低优先级的Downloader Middleware的process_request()和process_exception()方法就不会被继续调用了，转而依次开始执行每个Downloader Middleware的process_response()方法，调用完毕之后直接将Response对象发送给Spider来处理。
在这里我们直接返回了一个HtmlResponse对象，它是Response的子类，同样满足此条件，返回之后便会顺次调用每个Downloader Middleware的process_response()方法，而在process_response()中我们没有对其做特殊处理，接着他就会被发送给Spider，传给Request的回调函数进行解析。
到现在我们应该就能了解Downloader Middleware实现Selenium对接的原理了。
在settings.py里面开启它的调用：
  最后我们再实现一个Item Pipeline，将结果保存到MongoDB，实现如下：
 