b'K\xe8\xbf\x91\xe9\x82\xbb\xe7\xae\x97\xe6\xb3\x95\xe7\x9a\x84Python\xe5\xae\x9e\xe7\x8e\xb0' 作为『十大机器学习算法』之一的K-近邻（K-Nearest Neighbors）算法是思想简单、易于理解的一种分类和回归算法。今天，我们来一起学习KNN算法的基本原理，并用Python实现该算法，最后，通过一个案例阐述其应用价值。
（添加一个直观的图）
它基于这样的简单假设：彼此靠近的点更有可能属于同一个类别。用大俗话来说就是『臭味相投』,或者说『近朱者赤，近墨者黑』。
它并未试图建立一个显示的预测模型，而是直接通过预测点的临近训练集点来确定其所属类别。
K近邻算法的实现主要基于三大基本要素：
  K近邻算法的实施步骤如下：
根据给定的距离度量，在训练集TT中寻找出与xx最近邻的k个点，涵盖这k个点的x的邻域记作Nk(x);
在Nk(x)中根据分类决策规则决定样本的所属类别y:
http://mmbiz.qpic.cn/mmbiz_png/LiaGhAsRNttsicjGWS4XgSRgeWux1QdP513dIxef64mT541KMvoapabvkB8bia4m07mQ3GxeiamZsEzlxnJxcKTQ9w/0?wx_fmt=png
K近邻算法对K的选择非常敏感。K值越小意味着模型复杂度越高，从而容易产生过拟合；K值越大则意味着整体的模型变得简单，学习的近似近似误差会增大。
在实际的应用中，一般采用一个比较小的K值。并采用交叉验证的方法，选取一个最优的K值。
距离度量一般采用欧式距离。也可以根据需要采用LpLp距离或明氏距离。
K近邻算法中的分类决策多采用多数表决的方法进行。它等价于寻求经验风险最小化。
但这个规则存在一个潜在的问题：有可能多个类别的投票数同为最高。这个时候，究竟应该判为哪一个类别？
