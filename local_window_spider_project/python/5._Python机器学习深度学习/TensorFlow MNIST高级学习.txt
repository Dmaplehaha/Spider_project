b'TensorFlow MNIST\xe9\xab\x98\xe7\xba\xa7\xe5\xad\xa6\xe4\xb9\xa0' 使用<a href="http://mp.weixin.qq.com/s?__biz=MzI5NDY1MjQzNA==&amp;mid=2247484843&amp;idx=1&amp;sn=fab148a0e33ed6765bc1bd43601085ad&amp;chksm=ec5edad6db2953c01b5796d94cb02439d37de6fbd0c0a20898e56485271a4e31fd93c8dcd9b4&amp;scene=21#wechat_redirect" target="_blank" style="">上一节</a>的模型，在 MNIST 数据集上只有 92% 正确率，实在太糟糕。在这个小节里，我们用一个稍微复杂的模型：卷积神经网络来改善效果，这会达到大概99.2%的准确率。虽然不是最高，但是还是比较让人满意。<br style=""  />
为了创建这个模型，我们需要创建大量的权重和偏置项。这个模型中的权重在初始化时应该加入少量的噪声来打破对称性以及避免 0 梯度。由于我们使用的是 ReLU 神经元，因此比较好的做法是用一个较小的正数来初始化偏置项，以避免神经元节点输出恒为 0 的问题（dead neurons）。为了不在建立模型的时候反复做初始化操作，我们定义两个函数用于初始化。
 TensorFlow 在卷积和池化上有很强的灵活性。我们怎么处理边界？步长应该设多大？在这个实例里，我们会一直使用 vanilla 版本。我们的卷积使用1步长（stride size），0边距（padding size）的模板，保证输出和输入是同一个大小。我们的池化用简单传统的2x2大小的模板做 max pooling。为了代码更简洁，我们把这部分抽象成一个函数。
 tf.nn.conv2d 是 TensorFlow 里面实现卷积的函数，这是搭建卷积神经网络比较核心的一个方法，非常重要。
参数介绍如下：
input，指需要做卷积的输入图像，它要求是一个 Tensor，具有 [batch, in_height, in_width, in_channels] 这样的 shape，具体含义是 [训练时一个batch的图片数量, 图片高度, 图片宽度, 图像通道数]，注意这是一个 4 维的 Tensor，要求类型为 float32 和 float64 其中之一。
filter，相当于 CNN 中的卷积核，它要求是一个 Tensor，具有 [filter_height, filter_width, in_channels, out_channels] 这样的shape，具体含义是 [卷积核的高度，卷积核的宽度，图像通道数，卷积核个数]，要求类型与参数 input 相同，有一个地方需要注意，第三维 in_channels，就是参数 input 的第四维。
strides，卷积时在图像每一维的步长，这是一个一维的向量，长度 4。
padding，string 类型的量，只能是 SAME、VALID 其中之一，这个值决定了不同的卷积方式。
use_cudnn_on_gpu，布尔类型，是否使用 cudnn 加速，默认为true
实例如下：
  是CNN当中的最大值池化操作，其实用法和卷积很类似。
参数介绍如下：
value，需要池化的输入，一般池化层接在卷积层后面，所以输入通常是 feature map，依然是 [batch, height, width, channels] 这样的shape。
ksize，池化窗口的大小，取一个四维向量，一般是 [1, height, width, 1]，因为我们不想在 batch 和 channels 上做池化，所以这两个维度设为了1。
strides，和卷积类似，窗口在每一个维度上滑动的步长，一般也是 [1, stride,stride, 1]。
padding，和卷积类似，可以取 VALID、SAME，返回一个 Tensor，类型不变，shape 仍然是 [batch, height, width, channels] 这种形式。
示例如下：
假设有这样一张图，双通道 第一个通道：
http://mmbiz.qpic.cn/mmbiz_jpg/LiaGhAsRNtttWoppHGoB591iad8uSaSA5hoFj0FfRE36IUZoSvUVhumI1N0z4ibVJtDla10LH1ib8GK2WviaAtlibppA/0?wx_fmt=jpeg
第二个通道：
http://mmbiz.qpic.cn/mmbiz_jpg/LiaGhAsRNtttWoppHGoB591iad8uSaSA5hFJGhicKawkDWoNiavJCtUzgzyWKmIcicBLm897z72X76uoY9VwqD3AQoQ/0?wx_fmt=jpeg
用程序去做最大值池化，代码如下：
  http://mmbiz.qpic.cn/mmbiz_jpg/LiaGhAsRNtttWoppHGoB591iad8uSaSA5hUjiaRaiaEdFbfziaTL7ac2TO0qvYfcuGI0rY1NlHu3RuGyFGT7YMXvHJQ/0?wx_fmt=jpeg
http://mmbiz.qpic.cn/mmbiz_jpg/LiaGhAsRNtttWoppHGoB591iad8uSaSA5h4js5IibxTib5yic301iciaKqEpjia0yOEqk2bUTc4sta2AicHFXEgvTgj3Q0A/0?wx_fmt=jpeg
现在我们可以开始实现第一层了。它由一个卷积接一个 max pooling 完成。卷积在每个 5x5 的 patch 中算出 32 个特征。卷积的权重张量形状是 [5, 5, 1, 32]，前两个维度是 patch 的大小，接着是输入的通道数目，最后是输出的通道数目，而对于每一个输出通道都有一个对应的偏置量。
      为了进行训练和评估，我们使用与之前简单的单层 SoftMax 神经网络模型几乎相同的一套代码，只是我们会用更加复杂的 ADAM 优化器来做梯度最速下降，在 feed_dict 中加入额外的参数 keep_prob 来控制 dropout 比例。然后每 100次 迭代输出一次日志。
 运行结果：
    