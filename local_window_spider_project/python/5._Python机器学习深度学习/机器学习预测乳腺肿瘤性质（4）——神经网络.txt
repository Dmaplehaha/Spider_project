b'\xe6\x9c\xba\xe5\x99\xa8\xe5\xad\xa6\xe4\xb9\xa0\xe9\xa2\x84\xe6\xb5\x8b\xe4\xb9\xb3\xe8\x85\xba\xe8\x82\xbf\xe7\x98\xa4\xe6\x80\xa7\xe8\xb4\xa8\xef\xbc\x884\xef\xbc\x89\xe2\x80\x94\xe2\x80\x94\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c'    今天我们利用multiple layer perceptron （MLP）这种机器学习算法来分析乳腺肿瘤数据，MLP中文翻译为多层感知器， 是一种比较基础简单的人工神经网络(artificial neural network)算法。现在深度学习中最热门的卷积神经网络CNN（convolutional neural network）和循环神经网络RNN（recurrent neural network）都是以MLP为基础构建起来的，学习MLP可以帮助我们理解人工神经网络的基本知识和基础架构，为后面CNN和RNN的学习铺平道路。
首先我们来看看人工神经网络的基本结构：
https://mmbiz.qpic.cn/mmbiz_jpg/q5D147hKVM32iaPqY7iayH1X2JvIQibjvgN0dCbepichiaXh1N30pIMmetX6S0uZ6VHbtlLgxSibPnicpFheygs0icatCQ/640?wx_fmt=jpeg
简述一下神经网络的机制：把输入层的特征变量先乘以权重后求和加上一个偏置项，通过一个activation function 进行非线性转换，将这些非线性变换的结果再次作为输入层，以同样的方法再次进入下一个隐藏层，直到最后的输出层，该过程称为前向传播（forward propagation），再利用后向传播算法（back propagation）和梯度下降更新权重，不断最小化损失函数，直至得到最优解（或者局部最优解）。
神经网络结构的隐藏层部分可以自由搭建，就像乐高积木一样可以一层一层拼接起来，隐藏层可以10层，可以20层，甚至上百层上千层，只要算力足够，现在深度学习的发展趋势是网络层数越来越多，所以得名Deep Learning。每一个隐藏层的神经元个数也可自由设置，可以说神经网络的构建没有严格意义上的规则可循，但是网络结构常常决定最后的算法效果。CNN和RNN正是通过对基础神经网络架构的各项要素（各层输入变量、隐藏层的维度、非线性转换方式等方面）进行改造完善，近年来在图像识别、语音识别、自然语言等领域表现出令人惊叹的效果，证明了神经网络这种层级网络结构在捕捉数据背后隐藏规律方面的巨大威力。
在应用方面，MLP算法有一下优势：
   当然也有其不足之处<br style="margin: 0px;padding: 0px;max-width: 100%;box-sizing: border-box !important;overflow-wrap: break-word !important;"  />
     Python的scikit-learn Module提供了专门的函数构建MLP的分类器，即<strong style="margin: 0px;padding: 0px;max-width: 100%;box-sizing: border-box !important;overflow-wrap: break-word !important;">MLPclassifier</strong><br style="margin: 0px;padding: 0px;max-width: 100%;box-sizing: border-box !important;overflow-wrap: break-word !important;"  />
         solver是通过不断寻找损失函数最优解来进行权重迭代更新的优化器，有三种可以选择 {‘lbfgs’, ‘sgd’, ‘adam’}，默认设置是“adam”，‘lbfgs’ 是拟牛顿法的一种，.‘sgd’ 是随机梯度下降法，‘adam’ 是基于随机梯度下降的另一种优化算法，adam在大规模训练集上表现优秀（相对来说能够节省训练时间和提高validation score），lbfgs在小规模训练集上效果更好，收敛更快。
  http://mmbiz.qpic.cn/mmbiz_png/q5D147hKVM1d0re3tTPkSXAEpVvqu38uibHUCUSsaurlG2s8jVEDWnFMnADibBu69v1uKpGh1KwcU9wscoqkic3Mg/0?wx_fmt=gif
‘adaptive’会依据损失函数的值来调整学习率，只要训练时的损失函数值一直下降，学习率不变。但是如果连续两次迭代都没有降低损失函数一个tol的值，或者将validation score提高一个tol，在early_stopping参数为on的情况下，当前的学习率会除以五，‘adaptive’只在solver选择sgd的时候有效。
           