b'\xe6\x9c\xba\xe5\x99\xa8\xe5\xad\xa6\xe4\xb9\xa0\xe9\xa2\x84\xe6\xb5\x8b\xe4\xb9\xb3\xe8\x85\xba\xe8\x82\xbf\xe7\x98\xa4\xe6\x80\xa7\xe8\xb4\xa8\xef\xbc\x887\xef\xbc\x89'        机器学习称这种把机器学习模型集成起来的方法为ensemble learning（集成学习）。集成学习的原理很容易理解，非常intuitiv，就是把不同的机器学习模型组合起来，发挥各自的优势，扬长避短。集成学习有下面几个优势：
预测效果好，集成学习多数情况下优于单一模型的预测效果。在诸如kaggle这样的机器学习模型竞赛中，名列前茅的选手们往往无一例外使用ensemble learning的方法来提升模型的最终预测效果。
部分集成学习可以并列实现，在不同的cpu上跑不同的模型，然后将这些模型再集成起来
组合方法简单灵活，可以组合不同类型的机器学习模型，也可以组合同一类型的随机机器学习模型（典型案例就是决策树模型组合为随机森林），可以自己设定集成的规则。
集成各个学习器的结果主要是通过voting（投票）的方式。例如面对一个二分类问题（0和1）的时候，logistic回归给出一个分类结果1，svm预测的结果是0，朴素贝叶斯预测出来的是1，人工神经网络给出的是1，那么按照少数服从多数的原则，集成分类的预测结果就是1. 这在机器学习中称为hard voting 硬投票方式，还有soft voting 软投票方式，就是依据分类器给出的概率平均值结果来投票，比如A分类器预测的概率是{0：0.53，1:0.47}，B分类器预测出的概率是{0:0.36,1:0.64}，那分类为0的概率均值为（0.53+0.36）/2=0.445, 分类为1的概率为（0.47+0.64）/2=0.555，所以最终预测结果为分类1，当然soft voting仅限于那些能够给出分类概率的学习器。另外还可以给每个学习器设置不同权重，给效果较好的学习器赋予较大的权重，使其预测结果在最终决策中发挥作用更大。
下面介绍集成学习的两种主要形式：bagging and boosting，
bagging就是反复有放回（或无放回）在样本数据集中随机抽出子样本作为训练集，不断从这些子样本训练出模型，然后把所有模型结果集成。bagging不仅 可以抽取数据样例（instance），也可以随机抽取特征（feature）。 总之可以理解为在样本空间里不断提取亚空间出来做训练集不断训练模型，最后把众多个模型集成起来。前面文章里用到的Random Forest随机森林就是bagging的典型代表。
boosting也是把弱学习器集成为强学习器，和bagging的不同之处在于，大部分boosting方法强调有先后顺序的进行模型训练，从前一个学习器所犯的错误中学习，吃一堑长一智，不断迭代循环改进下一个学习器。然后再把所有的学习器按照一定的规则再集成其预测结果。boosting的两大主力分别为adaboosting和Gradient boosting。
